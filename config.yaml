Mistral:
  model_type: "mistral"
  model_config: {'max_new_tokens': 512,
                'context_length': 4096, 
                'gpu_layers': 0}
  model_path:
    small: "models/mistral-7b-v0.1.Q5_K_S.gguf"
    large: "models/mistral-7b-v0.1.Q5_K_S.gguf"
  stop_tokens: "Human:"

Llama2:
  model_type: "llama"
  model_config: {'max_new_tokens': 256,
                'repetition_penalty': 1.1, 
                'context_length': 4096,
                'temperature': 0.1}
  model_path:
    large: "models/llama-2-7b-chat.ggmlv3.q8_0.bin"
  stop_tokens: "Human:"

TinyLlama:
  model_name: "tinyllama"
  temperature: 0
  stop_tokens: ["<|system|>", "<|user|>", "<|assistant|>", "</s>"]

text_processing:
  separators: ["\n"]
  chunk_size: 1000
  chunk_overlap: 90

vector_db:
  huggingface_embeddings_path: "BAAI/bge-large-en-v1.5"
  ollama_embeddibg_path: "all-minilm"
  persist_directory: "vectorstore"

documents_path: "docs"

device: "GPU"